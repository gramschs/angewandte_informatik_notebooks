{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42c0220",
   "metadata": {},
   "source": [
    "# 13.3 Polynomiale Regression\n",
    "\n",
    "Die in den vorangegangenen Regressionstechniken erweitern wir nun von linearen\n",
    "Regressiongeraden auf Regressionspolynome wie beispielsweise\n",
    "Regressionsparabeln.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie können mit **polyfit** zu gegebenen Messdaten die Koeffizienten eines\n",
    "  Regressionspolynoms bestimmen.\n",
    "* Sie können mit **polyval** aus den Koeffizienten ein Regressionspolynom\n",
    "  auswerten.\n",
    "* Sie wissen, was die Begriffe **Underfitting** und **Overfitting** bedeuten.\n",
    "* Sie können mit dem Bestimmtheitsmaß R² abschätzen, welcher Polynomgrad $n$ zu\n",
    "  den Daten passt.\n",
    "\n",
    "## Regressionspolynome\n",
    "\n",
    "Ein Regressionspolynom ist eine Möglichkeit der Regressionsanalyse, bei der die\n",
    "Beziehung zwischen einer unabhängigen/erklärenden Variablen $x$ und einer\n",
    "abhängigen Variablen $y$ durch ein Polynom modelliert wird. Damit erweitert die\n",
    "polynomiale Regression die einfache lineare Regression, indem sie einen\n",
    "quadratischen oder kubischen Anteil berücksichtigt. Theoretisch sind noch höhere\n",
    "Polynomgrade möglich.\n",
    "\n",
    "Ein Polynom 2. Grades hat die Form\n",
    "\n",
    "$$y = ax^2 + bx + c,$$\n",
    "\n",
    "ein Polynom 3. Grades\n",
    "\n",
    "$$y = ax^3 + bx^2 + cx + d.$$\n",
    "\n",
    "Die reellen Zahlen $a, b, c, d$ werden Koeffizienten des Polynoms genannt.\n",
    "\n",
    "## Beispiel Regressionsparabel\n",
    "\n",
    "Wir betrachten als Beispiel die folgenden künstlichen Messwerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3072222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [-1, 0, 1, 2, 3, 4, 5]\n",
    "y = [5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23b6a9",
   "metadata": {},
   "source": [
    "Sieht nicht nach einer linearen Funktion, also einer Geraden aus. Wir probieren\n",
    "es mit einer quadratischen Funktion. Die Modellfunktion lautet\n",
    "\n",
    "$$f(x)=ax^2 + bx + c$$\n",
    "\n",
    "mit den Parametern $a$, $b$  und $c$. Wir setzen in der `polyfit()`-Funktion den\n",
    "Polynomgrad auf `grad=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = np.polyfit(x, y, 2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a38818",
   "metadata": {},
   "source": [
    "Bei der Zuordnung der Koeffizienten müssen wir sorgsam auf die Sortierung\n",
    "achten. Unsere Modellfunktion beginnt beim quadratischen Anteil $ax^2$, dann\n",
    "kommt der lineare Anteil $bx$ und zuletzt der konstante Part $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d161c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = p[0]\n",
    "b = p[1]\n",
    "c = p[2]\n",
    "\n",
    "print(f'a = {a}')\n",
    "print(f'b = {b}')\n",
    "print(f'c = {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c489a39",
   "metadata": {},
   "source": [
    "Wir erhalten also als Regressionsfunktion\n",
    "\n",
    "$$r(x)=-1.9 x^2 + 7.4 x + 14.5.$$\n",
    "\n",
    "Visualisieren wir die Modellfunktion zusammen mit den Messpunkten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.scatter(x,y);\n",
    "\n",
    "x_plot = np.linspace(-1, 5, 100);\n",
    "y_plot = a * x_plot**2 + b * x_plot + c\n",
    "plt.plot(x_plot, y_plot, color='red')\n",
    "\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815fb088",
   "metadata": {},
   "source": [
    "Alternativ kann die Funktion `polyval` dazu genutzt werden, um die Parabel auszuwerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.scatter(x,y);\n",
    "\n",
    "x_plot = np.linspace(-1, 5, 100);\n",
    "y_plot = np.polyval(p, x_plot)\n",
    "plt.plot(x_plot, y_plot, color='red')\n",
    "\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f8a8c",
   "metadata": {},
   "source": [
    "## Zu viel des Guten, höhere Polynomgrade sind nicht besser\n",
    "\n",
    "Regressionspolynome scheinen zunächst besser zu sein als Regressionsgeraden.\n",
    "Durch die zusätzlichen Terme können auch nichtlineare Beziehungen und komplexere\n",
    "Muster in den Daten erklärt werden. Allerdings birgt die Verwendung höherer\n",
    "Polynomgrade auch das Risiko des Overfittings. Der Begriff **Overfitting**\n",
    "bedeutet, dass das Regressionspolynom zu genau an die Daten angepasst wurde und\n",
    "neue Daten schlechter prognostiziert. Das Gegenteil davon ist **Underfitting**.\n",
    "Das Regressionspolynom hat einen zu kleinen/niedrigen Polynomgrad und kann daher\n",
    "die Daten kaum bis gar nicht erklären. Die Wahl des Polynomgrades ist daher sehr\n",
    "wichtig.\n",
    "\n",
    ":class: note\n",
    "Ist ein Modell zu stark an die Trainingsdaten angepasst und lässt es sich daher\n",
    "nicht mehr verallgemeinern, liegt **Overfitting** (Überanpassung) vor. Fehlen\n",
    "dahingegen erklärende Variablen, so dass die Komplexität der Daten nicht\n",
    "abgebildet werden kann, sprechen wir von **Underfitting** (Unteranpassung).\n",
    "\n",
    "Wir betrachten dazu das Beispiel von oben und verändern den Polynomgrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# künstliche Messdaten\n",
    "x = [-1, 0, 1, 2, 3, 4, 5]\n",
    "y = [5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y, color='black')\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "\n",
    "x_plot = np.linspace(-1,5)\n",
    "\n",
    "for grad in [1, 2, 3, 4]:\n",
    "  # berechne Regressionspolynom\n",
    "  p = np.polyfit(x, y, grad)\n",
    "  y_plot = np.polyval(p, x_plot)\n",
    "  # visualisiere zusätzlich das Regressionspolynon\n",
    "  plt.plot(x_plot, y_plot, label=f'Grad {grad}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b4702",
   "metadata": {},
   "source": [
    "Eine Regressionsgerade kann die Messdaten nicht gut erklären, aber die\n",
    "Regressionspolynome Grad 2 bis 4 passen sehr gut zu den künstlichen Messdaten\n",
    "des Beispiels. Probieren wir noch höhere Polynomgrade aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0931f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# künstliche Messdaten\n",
    "x = [-1, 0, 1, 2, 3, 4, 5]\n",
    "y = [5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y, color='black')\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "\n",
    "x_plot = np.linspace(-1,5, 200)\n",
    "\n",
    "for grad in [5, 6, 7, 8]:\n",
    "  # berechne Regressionspolynom\n",
    "  p = np.polyfit(x, y, grad)\n",
    "  y_plot = np.polyval(p, x_plot)\n",
    "  # visualisiere zusätzlich das Regressionspolynon\n",
    "  plt.plot(x_plot, y_plot, label=f'Grad {grad}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197be6fa",
   "metadata": {},
   "source": [
    "Grad 5 ist schon gut, aber die Regressionspolynome Grad 6 bis 8 scheinen perfekt\n",
    "zu sein. Allerdings gibt es auch eine Warnung, denn wenn mehr\n",
    "Polynomkoeffizienten da sind als Messpunkte, ist die Bestimmung der\n",
    "Polynomkoeffizienten nicht mehr eindeutig. Bei $n$ Datenpunkten ist ein Polynom\n",
    "vom Grad $n-1$ bereits eindeutig bestimmt und interpoliert alle Punkte exakt.\n",
    "\n",
    "Dennoch, jeder Messpunkt wird exakt von dem Regressionspolynom getroffen.\n",
    "Demnach müssten alle Residuen 0 sein und damit für das Bestimmtheitsmaß $R^2 =\n",
    "1$ gelten. Am besten lassen wir eine Tabelle für den Polynomgrad und das\n",
    "jeweilige Residuum ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de75d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for grad in range(1,9):\n",
    "    p = np.polyfit(x, y, grad)\n",
    "    y_modell = np.polyval(p, x)\n",
    "    r2 = r2_score(y, y_modell)\n",
    "    print(f'Polynomgrad {grad}: R2 = {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d2a12",
   "metadata": {},
   "source": [
    "Die mathematische Theorie wird in der Praxis bestätigt. Der $R^2$-Score hat ab\n",
    "dem Polynomgrad 6 den Wert 1. Wenn wir den Polynomgrad noch höher treiben,\n",
    "passiert etwas Seltsames, was aber nicht im $R^2$-Score zu entdecken ist,\n",
    "sondern in der Visualisierung der Regressionspolynome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# künstliche Messdaten\n",
    "x = [-1, 0, 1, 2, 3, 4, 5]\n",
    "y = [5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y, color='black')\n",
    "plt.xlabel('Ursache')\n",
    "plt.ylabel('Wirkung')\n",
    "plt.title('Künstliche Messdaten')\n",
    "\n",
    "x_plot = np.linspace(-1,5, 200)\n",
    "\n",
    "for grad in [9, 10, 15, 25]:\n",
    "  # berechne Regressionspolynom\n",
    "  p = np.polyfit(x, y, grad)\n",
    "  y_plot = np.polyval(p, x_plot)\n",
    "  # visualisiere zusätzlich das Regressionspolynon\n",
    "  plt.plot(x_plot, y_plot, label=f'Grad {grad}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af70c3",
   "metadata": {},
   "source": [
    "Die Polynome beginnen zwischen $x=4$ und $x=5$ immer höher zu schwingen. Es ist\n",
    "unplausibel, dass diese Polynome die Messdaten gut erklären. Daher empfiehlt es\n",
    "sich, möglichst den kleinsten Polynomgrad zu nehmen, der sehr gut, aber nicht\n",
    "perfekt ist. Bei der Tabelle der R2-Werte haben wir gesehen, dass der R2-Wert\n",
    "von 0.0054 (Grad 1) auf 0.9834 (Grad 2) springt. Danach sind aber keine\n",
    "wesentlichen Verbesserungen mehr erkennbar. Daher wählen wir Grad 2 als\n",
    "Regressionspolynom.\n",
    "\n",
    ":class: note\n",
    "In der Ingenieurspraxis sollte immer das einfachste Modell gewählt werden, das\n",
    "die Daten ausreichend gut beschreibt. Dies folgt dem Prinzip »Ockhams\n",
    "Rasiermesser« (siehe [Wikipedia →  Ockhams\n",
    "Rasiermesser](https://de.wikipedia.org/wiki/Ockhams_Rasiermesser)).\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben wir gelernt, Messdaten durch Regressionspolynome zu modellieren. Dabei ist die sorgsame Wahl des Polynomgrads sehr wichtig, um Overfitting und Underfitting zu vermeiden. Es gibt außer den Regressionsmodellen noch viele weitere Möglichkeiten, für Messdaten Modelle zu erstellen. Damit beschäftigt sich das maschinelle Lernen, das ein Teilgebiet der Künstlichen Intelligenz ist."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
